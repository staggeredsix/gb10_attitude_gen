# Cluster-Accelerated Mind Mirror

This document captures the GPU-cluster version of the "Mind Mirror" demo: a two-node pipeline that fuses real-time facial expression analysis with diffusion-based portrait generation. The goal is to showcase cross-node tensor passing over dual 100 Gb links and deliver a 10–20 FPS animated experience on a large display.

## High-level flow

1. **Capture (Client/Edge):** Webcam frames are streamed (via WebRTC or WebSocket) to a small ingest service that performs light normalization (resize to 720p, RGB, per-channel mean/variance) and forwards batches to the vision node.
2. **Vision on DGX #1:**
   - A ViT-style VLM (Qwen-VL, NV-VLM, or OpenVLA) detects faces and classifies emotions.
   - Output tensors include face embeddings, bounding boxes, and top-*k* emotions with confidence.
   - Triton Inference Server hosts the VLM as an ensemble with MediaPipe face detection (optional) to pre-crop faces.
3. **Tensor handoff:**
   - Embeddings + emotion logits are serialized as GPU-resident tensors and sent to DGX #2 over **2×100 Gb** fabric using Triton’s Multi-Node Backend or gRPC+NCCL point-to-point.
   - NCCL is used to maintain stream-ordered, zero-copy transfers where possible; gRPC metadata carries prompts/emotion tags.
4. **Diffusion on DGX #2:**
   - A diffusion engine (SDXL-Turbo, FLUX.1, or LCM) consumes the emotion-conditioned prompt and optional face embedding.
   - TensorRT-LLM/TensorRT backends run the UNet/VAE with low-latency settings (reduced steps, classifier-free guidance tuned for speed, fp16/bf16).
   - Outputs are pushed into a circular buffer for the display service.
5. **Display:**
   - A lightweight streamer composites the animated portraits at **10–20 FPS** and renders them on the large screen (OpenGL/SDL or browser canvas via WebRTC data channel).

## Why this highlights the hardware

- **Cross-node tensor passing:** Demonstrates line-rate usage of the dual 100 Gb links by shipping GPU tensors instead of host-bound JSON payloads.
- **Elastic multi-GPU behavior:** Each DGX specializes (vision vs. diffusion) but acts like one aggregated accelerator for the user.
- **Highly visual:** Attendees see themselves transformed in real time, with visible morphs as their expression changes.

## Detailed architecture

### Node roles

- **DGX #1 (Vision):** Triton ensemble: MediaPipe face detector → VLM (Qwen-VL / NV-VLM / OpenVLA) → small head that emits emotion logits and an optional CLIP/ViT embedding.
- **DGX #2 (Diffusion):** Triton model repository with SDXL-Turbo/FLUX.1/LCM engines compiled via TensorRT-LLM. Includes a prompt-builder microservice that maps emotions to surreal style templates.
- **Ingest/Display:** Can run on DGX #2 or a separate edge box. Responsible for browser ingest, queueing, and rendering to the screen.

### Data contracts

- **Emotion payload:** `{face_id, timestamp, bbox, emotion_probs[8], embedding (optional), quality_score}`.
- **Prompt payload:** `{"subject": "attendee", "emotion": "joy", "style": "surreal neon"}` generated by a small ruleset or template library.
- **Diffusion input:** Prompt text + optional face embedding fused via cross-attention or ControlNet/LoRA adapter.

### Transport options

- **Triton Multi-Node Backend (preferred):** Configure an ensemble where stage 1 (vision) resides on DGX #1 and stage 2 (diffusion) on DGX #2; tensors are forwarded over RDMA without returning to host.
- **gRPC + NCCL P2P:** A thin client on DGX #1 issues async inference calls to DGX #2 with serialized GPU buffers; NCCL handles device-to-device transfers, and gRPC carries control metadata (timestamps, emotion labels, prompt strings).

### Performance targets

- **Frame ingest:** 30 FPS input, batched into micro-batches of 2–4 frames for the VLM.
- **Emotion refresh:** 10–20 FPS output with generation interval of 50–100 ms per portrait chunk (use lower diffusion steps and denoising strengths).
- **Latency budget:**
  - Capture/encode: ~5–10 ms
  - Vision inference: ~15–25 ms (fp16, batch 4)
  - Cross-node transfer: ~5–8 ms over 2×100 Gb
  - Diffusion: ~30–60 ms with SDXL-Turbo/LCM low-step configs
  - Display/composite: ~5 ms
- **Throughput protection:** Use token-bucket to cap diffusion regenerations per attendee; skip updates when emotion logits remain stable to reduce GPU load.

### Resilience and quality

- **Backpressure:** NCCL streams propagate queue depth; ingest drops frames when vision queue exceeds threshold.
- **Fallback:** If DGX #2 is saturated, DGX #1 can render a low-fi portrait using a tiny LCM on the same node.
- **Warmup:** Pre-run diffusion pipelines with canonical prompts to fill kernels and CUDA graphs.
- **Safety:** Optional NSFW filter on prompts before diffusion; clamp prompt templates to curated styles.

### Observability

- **Metrics:** Export Prometheus counters for per-stage latency, cross-node bandwidth, diffusion steps, and drop rates.
- **Tracing:** Enable OpenTelemetry spans across Triton requests to visualize the cross-node pipeline.
- **Visualization:** On-screen FPS meter and latency histogram to help staff narrate the demo.

## Implementation checklist

- [ ] Define Triton model repositories for VLM and diffusion with TensorRT-LLM plans.
- [ ] Implement the prompt-builder microservice with emotion→style templates.
- [ ] Wire Triton Multi-Node Backend (or gRPC+NCCL) for tensor handoff.
- [ ] Build a display compositor (browser canvas or SDL) that streams portraits at 10–20 FPS.
- [ ] Add monitoring dashboards for latency and bandwidth during the demo.

## Automation scripts

- **Single-node path:** `./scripts/single_spark.sh` builds the image and starts the browser-oriented web UI on one GPU host.
- **Dual-node orchestration:** `VISION_HOST=<dgx1> DIFFUSION_HOST=<dgx2> ./scripts/dual_spark.sh` will:
  - Bring up both 100G ports on each host, set MTU (default 9000), and print link state.
  - Sync this repo to both hosts (default `/opt/ai-mood-mirror`).
  - Build the container on each host and launch the service with the host-specific `ROLE` environment variable set.
  - Run lightweight GPU presence checks (`nvidia-smi`) plus TCP reachability tests in both directions on the selected web port.

## Mapping to the existing app

- Reuse the existing emotion mapping logic and prompt builder as the ruleset for the cluster version.
- Replace the local webcam loop with a browser ingest that feeds DGX #1 via WebRTC/WebSocket → Triton.
- Swap the single-node diffusion call with the DGX #2 Triton endpoint, preserving the throttling logic used in the current app.

## Demo script

1. Audience member steps up; browser captures their webcam feed and shows the raw video.
2. Vision overlay shows bounding boxes and detected emotion with confidence on the left screen.
3. On the right screen, the surreal portrait animates in sync with expression changes at 10–20 FPS.
4. Staff points out the cross-node bandwidth meter and low-latency stats to highlight the hardware.
